The purpose of this analysis is to make up a prediction model where we will be able to predict whether a recommendation is positive or negative. In this analysis, we will not focus on the Score, but only the positive/negative sentiment of the recommendation.

import pandas as pd
import numpy as np

np.random.seed(0)

def read_text_file(f):
    df_complete = pd.read_csv(f)
    #df = df_complete.loc[:,["Text","Score"]]
    #df.dropna(how="any", inplace=True)    
    return df_complete

df = read_text_file("../input/Reviews.csv")
print (df.head())
   Id   ProductId          UserId                      ProfileName  \
0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   
1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   
2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres "Natalia Corres"   
3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   
4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham "M. Wassir"   

   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \
0                     1                       1      5  1303862400   
1                     0                       0      1  1346976000   
2                     1                       1      4  1219017600   
3                     3                       3      2  1307923200   
4                     0                       0      5  1350777600   

                 Summary                                               Text  
0  Good Quality Dog Food  I have bought several of the Vitality canned d...  
1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  
2  "Delight" says it all  This is a confection that has been around a fe...  
3         Cough Medicine  If you are looking for the secret ingredient i...  
4            Great taffy  Great taffy at a great price.  There was a wid...  
df.shape

def partition(x):
    if x < 3:
        return 'negative'
    return 'positive'

Score = df['Score']
Score = Score.map(partition)

df['Review'] = df['Score'].map(partition)

print(df.head())
   Id   ProductId          UserId                      ProfileName  \
0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   
1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   
2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres "Natalia Corres"   
3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   
4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham "M. Wassir"   

   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \
0                     1                       1      5  1303862400   
1                     0                       0      1  1346976000   
2                     1                       1      4  1219017600   
3                     3                       3      2  1307923200   
4                     0                       0      5  1350777600   

                 Summary                                               Text  \
0  Good Quality Dog Food  I have bought several of the Vitality canned d...   
1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   
2  "Delight" says it all  This is a confection that has been around a fe...   
3         Cough Medicine  If you are looking for the secret ingredient i...   
4            Great taffy  Great taffy at a great price.  There was a wid...   

     Review  
0  positive  
1  negative  
2  positive  
3  negative  
4  positive  
Exploratory analysis
Distribution of ratings

I first looked at the distribution of ratings among all of the reviews. We see that 5-star reviews constitute a large proportion (64%) of all reviews. The next most prevalent rating is 4-stars(14%), followed by 1-star (9%), 3-star (8%), and finally 2-star reviews (5%).

df_count_prcnt = df.Score.value_counts()

def compute_percentage(x):
    pct = float(x/df_count_prcnt.sum()) * 100
    return round(pct, 2)

df_count_prcnt = df_count_prcnt.apply(compute_percentage)

df_count_prcnt.plot(kind="bar", colormap='jet')

print(df_count_prcnt)
5    63.88
4    14.19
1     9.19
3     7.50
2     5.24
Name: Score, dtype: float64

Exploratory analysis
Distribution of ratings

5-star reviews constitute a large proportion (64%) of all reviews. The next most prevalent rating is 4-stars(14%), followed by 1-star (9%), 3-star (8%), and finally 2-star reviews (5%).

# frequency counts for users|product etc.

def top_n_counts (n, col, col_1):
    gb = df.groupby(col)[col_1].count()
    gb = gb.sort_values(ascending=False)
    return gb.head(n)
     
top_n_counts(15, ['ProductId','UserId'], 'ProductId')
ProductId   UserId        
B00008CQVA  A29JUMRL1US6YP    11
B000084EZ4  A29JUMRL1US6YP    11
B000WFORH0  A29JUMRL1US6YP    11
B000WFRQQ4  A29JUMRL1US6YP    11
B000WFKWDI  A29JUMRL1US6YP    11
B000WFEN74  A29JUMRL1US6YP    11
B000WFPJIG  A29JUMRL1US6YP    11
B000WFKI82  A29JUMRL1US6YP    11
B000WFU8O6  A29JUMRL1US6YP    11
B000WFN0VO  A29JUMRL1US6YP    11
B000WFUL3E  A29JUMRL1US6YP    11
B000084DWM  A3TVZM3ZIXG8YW    10
B003MWBFXY  A3TVZM3ZIXG8YW    10
B003MA8P02  A3TVZM3ZIXG8YW    10
B003WK0D8O  A3TVZM3ZIXG8YW    10
Name: ProductId, dtype: int64
top_n_counts(15, ['UserId'], 'UserId').plot(kind='bar', figsize=(20,10), colormap='winter')

print (top_n_counts(15, ['UserId'], 'UserId'))
UserId
A3OXHLG6DIBRW8    448
A1YUL9PCJR3JTY    421
AY12DBB0U420B     389
A281NPSIMI1C2R    365
A1Z54EM24Y40LL    256
A1TMAVN4CEM8U8    204
A2MUGFV2TDQ47K    201
A3TVZM3ZIXG8YW    199
A3PJZ8TU8FDQ1K    178
AQQLWCMRNDFGI     176
A2SZLNSI5KOQJT    175
A29JUMRL1US6YP    172
AZV26LP92E6WU     167
AY1EF0GOH80EK     162
A31N6KB160O508    162
Name: UserId, dtype: int64

Need to understand what this is .. why there are multiple rows for same user, product and comment?

df[(df['ProductId'] == 'B000WFN0VO') & (df['UserId'] == 'A29JUMRL1US6YP')][['Text','Score']]
Text	Score
191484	The pet food industry can be one of the most i...	5
191527	The pet food industry can be one of the most i...	5
191533	The pet food industry can be one of the most i...	5
191536	The pet food industry can be one of the most i...	5
191543	The pet food industry can be one of the most i...	5
191547	The pet food industry can be one of the most i...	5
191560	The pet food industry can be one of the most i...	5
191561	The pet food industry can be one of the most i...	5
191568	The pet food industry can be one of the most i...	5
191593	The pet food industry can be one of the most i...	5
191597	The pet food industry can be one of the most i...	5
# Time series for monthly review counts
df['datetime'] = pd.to_datetime(df["Time"], unit='s')
df_grp = df.groupby([df.datetime.dt.year, df.datetime.dt.month, df.Score]).count()['ProductId'].unstack().fillna(0)


df_grp.plot(figsize=(20,10), rot=45, colormap='jet')
<matplotlib.axes._subplots.AxesSubplot at 0x7f41289de780>

# Time series for monthly review counts
df['datetime'] = pd.to_datetime(df["Time"], unit='s')
df_grp = df.groupby([df.datetime.dt.year, df.datetime.dt.month, df.Score]).count()['ProductId'].unstack()

df_grp.plot(kind="bar",figsize=(30,10), stacked=True, colormap='jet')
<matplotlib.axes._subplots.AxesSubplot at 0x7f4128a34e10>

pos = df.loc[df['Review'] == 'positive']
pos = pos[0:25000]

neg = df.loc[df['Review'] == 'negative']
neg = neg[0:25000]
import nltk
from nltk.corpus import stopwords
from wordcloud import WordCloud
import string
import matplotlib.pyplot as plt

def create_Word_Corpus(df):
    words_corpus = ''
    for val in pos["Summary"]:
        text = val.lower()
        #text = text.translate(trantab)
        tokens = nltk.word_tokenize(text)
        tokens = [word for word in tokens if word not in stopwords.words('english')]
        for words in tokens:
            words_corpus = words_corpus + words + ' '
    return words_corpus
        
# Generate a word cloud image
pos_wordcloud = WordCloud(width=900, height=500).generate(create_Word_Corpus(pos))
neg_wordcloud = WordCloud(width=900, height=500).generate(create_Word_Corpus(neg))

# Plot cloud
def plot_Cloud(wordCloud):
    plt.figure( figsize=(20,10), facecolor='k')
    plt.imshow(wordCloud)
    plt.axis("off")
    plt.tight_layout(pad=0)
    plt.show()
    plt.savefig('wordclouds.png', facecolor='k', bbox_inches='tight')
plot_Cloud(pos_wordcloud)

<matplotlib.figure.Figure at 0x7f41008b4e48>
plot_Cloud(neg_wordcloud)

<matplotlib.figure.Figure at 0x7f4100918be0>
def sampling_dataset(df):
    count = 5000
    class_df_sampled = pd.DataFrame(columns = ["Score","Text", "Review"])
    temp = []
    for c in df.Score.unique():
        class_indexes = df[df.Score == c].index
        random_indexes = np.random.choice(class_indexes, count, replace=False)
        temp.append(df.loc[random_indexes])
        
    for each_df in temp:
        class_df_sampled = pd.concat([class_df_sampled,each_df],axis=0)
    
    return class_df_sampled

df_Sample = sampling_dataset(df.loc[:,["Score","Text","Review"]])
df_Sample.reset_index(drop=True,inplace=True)
print (df_Sample.head())
print (df_Sample.shape)
   Score                                               Text    Review
0    5.0  ...at least I hope it remains available. It is...  positive
1    5.0  My grandson is a beef jerky addict. We don't h...  positive
2    5.0  This coffee has a really great taste asnd if v...  positive
3    5.0  Healthy edibles are a great treat for our 5 ye...  positive
4    5.0  I have never been one to buy chicken in a can ...  positive
(25000, 3)
from nltk import clean_html
from nltk import PorterStemmer
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
import re

stop = stopwords.words('english')

def Tokenizer(df):
    comments = []
    for index, datapoint in df.iterrows():
        # Strips punctuation/abbr, converts to lowercase
        text = re.sub('<[^>]*>', '', datapoint["Text"])
        emoticons = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)',text.lower())
        text = re.sub('[\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')
        # Tokenizes into sentences
        tokenized_words = [w for w in text.split() if w not in stop]
        comments.append(tokenized_words)
    df["clean_reviews"] = comments
    return df

def stemming(df):
    # Stem all words with stemmer
    comments = []
    for index, datapoint in df.iterrows():
        # Strips punctuation/abbr, converts to lowercase
        text = re.sub('<[^>]*>', '', datapoint["Text"])
        emoticons = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)',text.lower())
        text = re.sub('[\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')
        # Stemming
        porter = PorterStemmer()
        stem_words = [porter.stem(w) for w in text.split() if w not in stop]
        comments.append(stem_words)
    df["clean_reviews"] = comments
    return df
df_Sample = stemming(df_Sample)

print (df_Sample.head())
print (df_Sample.shape)
   Score                                               Text    Review  \
0    5.0  ...at least I hope it remains available. It is...  positive   
1    5.0  My grandson is a beef jerky addict. We don't h...  positive   
2    5.0  This coffee has a really great taste asnd if v...  positive   
3    5.0  Healthy edibles are a great treat for our 5 ye...  positive   
4    5.0  I have never been one to buy chicken in a can ...  positive   

                                       clean_reviews  
0  [least, hope, remain, avail, delici, way, get,...  
1  [grandson, beef, jerki, addict, much, choos, o...  
2  [coffe, realli, great, tast, asnd, smooth, str...  
3  [healthi, edibl, great, treat, 5, year, old, y...  
4  [never, one, buy, chicken, quit, foodi, day, s...  
(25000, 4)
from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize the "CountVectorizer" object, which is scikit-learn's bag of words tool.  
# min_df=5, discard words appearing in less than 5 documents
# max_df=0.8, discard words appering in more than 80% of the documents
# sublinear_tf=True, use sublinear weighting
# use_idf=True, enable IDF
vectorizer = TfidfVectorizer(min_df=5,
                             max_df = 0.8,
                             sublinear_tf=True,
                             use_idf=True)

train_vectors = vectorizer.fit_transform(df_Sample["Text"])
feature_names = vectorizer.get_feature_names()
# Take a look at the words in the vocabulary
vocab = vectorizer.get_feature_names()
print (vocab[1:200])
['000', '01', '02', '03', '04', '05', '06', '07', '08', '09', '0g', '0mg', '0z', '10', '100', '1000', '100g', '100mg', '105', '10g', '10lbs', '10mg', '10oz', '10th', '10x', '11', '110', '11g', '12', '120', '12g', '12oz', '13', '130', '135', '13g', '14', '140', '14g', '15', '150', '1500', '15g', '15mg', '16', '160', '16g', '16oz', '16th', '17', '170', '175', '17g', '18', '180', '18g', '19', '190', '1950', '1980', '1987', '1994', '1997', '19g', '1g', '1lb', '1oz', '1st', '20', '200', '2000', '2006', '2007', '2008', '2009', '200mg', '2010', '2011', '2012', '2013', '2014', '20g', '20mg', '20oz', '20th', '21', '210', '22', '220', '22g', '23', '24', '240', '24oz', '25', '250', '250ml', '25g', '26', '260', '27', '270', '28', '29', '2g', '2lb', '2nd', '2oz', '2x', '30', '300', '30g', '30lbs', '30th', '31', '32', '32oz', '33', '33g', '34', '35', '350', '35g', '36', '360', '365', '37', '38', '39', '3g', '3lb', '3oz', '3rd', '3x', '40', '400', '41', '42', '420', '43', '44', '45', '46', '47', '48', '49', '4g', '4health', '4lb', '4oz', '4th', '4x', '50', '500', '51', '52', '53', '54', '55', '56', '57', '58', '59', '5g', '5lb', '5oz', '5th', '60', '600', '61', '62', '63', '64', '65', '66', '67', '68', '69', '6g', '6oz', '6th', '6x', '70', '700', '71', '72', '73', '73beats', '74', '75', '77', '78', '79', '7g', '7oz', '7th', '80', '800', '81']
Train Random forest classifier on a grid search parameters

Training RFC classifier with grid search to tune hyperparameters . Score on test data and highest CV score printed.

print ("Training the random forest...")
from sklearn.ensemble import RandomForestClassifier

# Initialize a Random Forest classifier with 100 trees
forest = RandomForestClassifier(n_estimators = 100) 

# Fit the forest to the training set, using the bag of words as 
# features and the sentiment labels as the response variable
#
# This may take a few minutes to run
forest = forest.fit( train_vectors, df_Sample["Review"] )

# prediction_rbf = classifier_rbf.predict(test_vectors)
Training the random forest...
df_TestSample = sampling_dataset(df.loc[:,["Score","Text","Review"]])
df_TestSample.reset_index(drop=True,inplace=True)

print (df_TestSample.head())
print (df_TestSample.shape)
   Score                                               Text    Review
0    5.0  I debated buying this, as it's rather expensiv...  positive
1    5.0  We love making popcorn on Friday evenings...af...  positive
2    5.0  I absolutely love these chips.  All the flavor...  positive
3    5.0  I just ordered this product and tried my 1st p...  positive
4    5.0  So easy to make, I took the sauce & added abou...  positive
(25000, 3)
test_vectors = vectorizer.transform(df_TestSample["Review"])
prediction_rbf = forest.predict(test_vectors)
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC, NuSVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.metrics import accuracy_score
Classifiers = [
    LogisticRegression(C=0.000000001,solver='liblinear',max_iter=200),
    #KNeighborsClassifier(3),
    #SVC(kernel="rbf", C=0.025, probability=True),
    DecisionTreeClassifier(),
    #RandomForestClassifier(n_estimators=200),
    #AdaBoostClassifier(),
    #GaussianNB()
]
dense_features=train_vectors.toarray()
dense_test= test_vectors.toarray()

Accuracy=[]
Model=[]
for classifier in Classifiers:
    print('training '+classifier.__class__.__name__)
    try:
        fit = classifier.fit(train_vectors,df_Sample["Review"])
        pred = fit.predict(test_vectors)
    except Exception:
        fit = classifier.fit(dense_features,df_Sample["Review"])
        pred = fit.predict(dense_test)
    accuracy = accuracy_score(pred,df_TestSample["Review"])
    Accuracy.append(accuracy)
    Model.append(classifier.__class__.__name__)
    print('Accuracy of '+classifier.__class__.__name__+'is '+str(accuracy))    
training LogisticRegression
Accuracy of LogisticRegressionis 0.6
training DecisionTreeClassifier
Accuracy of DecisionTreeClassifieris 0.4
#Compare the model performances
Index = [1,2]
plt.bar(Index,Accuracy)
plt.xticks(Index, Model,rotation=45)
plt.ylabel('Accuracy')
plt.xlabel('Model')
plt.title('Accuracies of Models')
<matplotlib.text.Text at 0x7f40fc7cc400>
